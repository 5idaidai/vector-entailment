\begin{abstract}
  Supervised recursive neural network models (RNNs) for sentence
  meaning have been successful in a wide array of sophisticated
  language tasks, but it remains an open question whether they can
  achieve the same results as compositional semantic grammars based in
  logical forms. We address this question directly by using a logical
  grammar to generate controlled data sets encoding the relationships
  (e.g., entailment, contradiction) between pairs of expressions and
  evaluating whether each of two classes of neural model --- plain
  RNNs and recursive neural tensor networks (RNTNs) --- can learn
  those relationships correctly. Our first experiment evaluates
  whether these models can learn the basic algebra of logical
  relations involved. Our second and third experiments extend this
  evaluation to complex recursive structures and sentences involving
  quantification.  We find that the plain RNN achieves only mixed
  results on all three experiments, whereas the stronger RNTN models
  generalizes well in every setting.
\end{abstract}