\begin{abstract}
  Supervised recursive neural network models (RNNs) for sentence
  meaning have been successful in a wide array of sophisticated
  language tasks, but it remains an open question whether succeed in
  learning compositional semantic grammars that allow logical deduction. 
  We address this question directly by using a logical
  grammar to generate controlled data sets encoding the relationships
  (e.g., entailment, contradiction) between pairs of expressions and
  evaluating whether each of two classes of neural model --- plain
  RNNs and recursive neural tensor networks (RNTNs) --- can learn
  those relationships correctly. Our first experiment evaluates
  whether these models can learn the basic algebra of logical
  relations involved. Our second and third experiments extend this
  evaluation to complex recursive structures and sentences involving
  quantification.  We find that the plain RNN achieves only mixed
  results on all three experiments, whereas the stronger RNTN models
  generalizes well in every setting. This serves as compelling evidence that
  RNTNs provide a satisfactory representation of meaning for
  natural language reasoning.
\end{abstract}