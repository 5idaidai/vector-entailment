\section{Relation composition}\label{sec:join}

If a model is to learn the behavior of a relational logic like the one
presented here from a finite amount data, it must be able to learn to
deduce new relations from seen relations in a sound and complete
manner. The simplest such deductions involve atomic statements using
the relations in Table~\ref{b-table}. For instance, given that $a
\natrev b$ and $b \natrev c$, one can conclude that $a \natrev c$, by
basic set-theoretic reasoning (transitivity of $\natrev$). Similarly,
from $a \natfor b$ and $b \natneg c$, it follows that $a \natalt c$.
The full set of sound inferences of this form is summarized in
Table~\ref{tab:jointable}; cells containing a dot correspond to pairs
of relations for which no valid inference can be drawn in our logic.

% about the relations themselves that do not depend on the
% internal structure of the things being compared. For example, given
% that $a\sqsupset b$ and $b\sqsupset c$ one can conclude that
% $a\sqsupset c$ by the transitivity of $\sqsupset$, even without
% understanding $a$, $b$, or $c$. These seven relations support more
% than just transitivity: MacCartney and Manning's
% \cite{maccartney2009extended} join table defines 32 valid inferences
% that can be made on the basis of pairs of relations of the form $a R
% b$ and $b R' c$, including several less intuitive ones such as that if
% $a \natneg b$ and $b~|~c$ then $a \sqsupset c$.

\begin{table}[htp]
  \centering  
  \setlength{\arraycolsep}{8pt}
  \renewcommand{\arraystretch}{1.1}
  \newcommand{\UNK}{\cdot}  
  $\begin{array}[t]{c@{ \ }|*{7}{c}|}
    %\hline
    \multicolumn{1}{c}{}
             & \nateq     & \natfor     & \natrev     & \natneg    & \natalt     & \natcov     & \multicolumn{1}{c}{\natind} \\
    \cline{2-8}
    \nateq  & \nateq &   \natfor &  \natrev &  \natneg &   \natalt &  \natcov &  \natind \\
    \natfor & \natfor &  \natfor &  \UNK &  \natalt &   \natalt &  \UNK &  \UNK \\
    \natrev & \natrev &  \UNK &  \natrev &  \natcov &   \UNK &  \natcov &  \UNK \\
    \natneg & \natneg &  \natcov &  \natalt &  \nateq &    \natrev &  \natfor &  \natind \\
    \natalt & \natalt &  \UNK &  \natalt &  \natfor &   \UNK &  \natfor &  \UNK \\
    \natcov & \natcov &  \natcov &  \UNK &  \natrev &   \natrev &  \UNK &  \UNK \\
    \natind & \natind & \UNK &  \UNK &  \natind &  \UNK &  \UNK &  \UNK \\
    \cline{2-8}
  \end{array}$
  \caption{Full set of inferences between natural logic relations.
    Given $a \mathbin{R} b$ and $b \mathbin{S} c$, where $R$ and $S$
    are the row and column relations, respectively, and $a$, $b$, and
    $c$ are arbitrary formulae, the table provides the relation
    $a \mathbin{T} c$.}
  \label{tab:jointable}
\end{table}

To test our models' ability to learn this relational structure, we
create small models for our logic in which terms denote sets of
entities from a single domain of seven entities (integers).
Figure~\ref{lattice-figure} depicts a small model of this form. The
lattice structure gives the full model, in which all the statements on
the right can be proved. The highlighted nodes represent a
hypothetical training set drawn from this larger space, which allows
us to prove only a subset of the full set of true statements.

\begin{figure}[htp]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \newcommand{\labelednode}[4]{\put(#1,#2){\oval(1.5,1)}\put(#1,#2){\makebox(0,0){$\begin{array}{c}#3\\\{#4\}\end{array}$}}}
    \setlength{\unitlength}{1cm}
    \begin{picture}(5,5.5)
      \labelednode{2.50}{5}{}{1,2,3}
      
      \put(0.75,4){\line(3,1){1.5}}
      \put(2.5,4){\line(0,1){0.5}}
      \put(4.25,4){\line(-3,1){1.5}}
      
      \labelednode{0.75}{3.5}{a,b}{1,2}
      \labelednode{2.50}{3.5}{c}{1,3}
      \labelednode{4.25}{3.5}{d}{2,3}
      
      \put(0.75,2.5){\line(0,1){0.5}}
      \put(0.75,2.5){\line(3,1){1.5}}
      
      \put(2.5,2.5){\line(-3,1){1.5}}
      \put(2.5,2.5){\line(3,1){1.5}}
      
      \put(4.25,2.5){\line(0,1){0.5}}
      \put(4.25,2.5){\line(-3,1){1.5}}
      

      \labelednode{0.75}{2}{e,f}{1}
      \labelednode{2.50}{2}{}{2}
      \labelednode{4.25}{2}{g,h}{3}
      
      \put(2.5,1){\line(-3,1){1.5}}
      \put(2.5,1){\line(0,1){0.5}}
      \put(2.5,1){\line(3,1){1.5}}
      
      \labelednode{2.5}{0.5}{}{}
    \end{picture}
    \caption{Simple boolean model. The letters name the sets. Not all sets have names, and
    some sets have multiple names, so that learning $\nateq$ is non-trivial.}
  \end{subfigure}
  \qquad
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \setlength{\tabcolsep}{12pt}
    \begin{tabular}[b]{c  c}
      \toprule
      Train & Test \\
      \midrule
                    & $b \nateq b$ \\
      $b \natcov c$ &               \\
                    & $b \natcov d$ \\
                    & \strikeout{$b \natrev e$} \\
      $c \natcov d$ &               \\
      $c \natrev e$ &               \\
                    & \strikeout{$c \nateq f$} \\
      $c \natrev g$ &               \\ 
                    & $e \natfor b$ \\
      $e \natfor c$ &               \\[-1ex]
      $\vdots$      & $\vdots$ \\
      \bottomrule
    \end{tabular}

    \caption{An example train/test split from the full set
      of statements one can make about the model.
      Statements not provable from the test data are crossed out.}
  \end{subfigure}  
  % \begin{tabular}{ll|lll}
  %   $a := \{1, 3, 4, 5, 6\}	$&~~~&~~~& $a \equiv a$	    	& $a~\#~c$	\\
  %   $b := \{0, 3, 5, 6\}	$&~~~&~~~& $e \sqsupset c$	&$b \smallsmile c$		\\
  %   $c := \{1, 2, 3, 4\}	$&~~~&~~~& $d \smallsmile e$	& $b~\#~d$		\\
  %   $d := \{0, 4\}		$&~~~&~~~& $a \sqsubset e$	& $ d \equiv d$	\\
  %   $e := \{1, 2, 3, 4, 5, 6\}$&~~~&~~~& $a~\#~b$		& ... 	\\
  % \end{tabular}
  \caption{Some sample randomly generated sets, and some of the relations defined between them.}
  \label{lattice-figure}
\end{figure}

% We test the model's ability to learn this behavior by creating
% artificial data sets of terms which represent sets of numbers. Since
% MacCartney and Manning's set of relations hold between sets as well as
% between sentences, we can use the underlying set structure to generate
% the all of the relations that hold between any pair of these terms, as
% in Figure \ref{lattice-figure}. We train the model defined above on a
% subset of these relations, but rather then presenting the model with a
% pair of tree-structured sentences as inputs, simply present it with
% two single terms, each of which corresponds to a single vector in the
% (randomly initialized) vocabulary matrix $V$, ensuring that the model
% has no information about the terms being compared except the relations
% between them.

In our experiments, we create 80 randomly generated sets drawing from
the same domain of seven element. This yields a data set consisting of
6400 statements about pairs of entities. 3200 of these pairs are
chosen as a test set, and that test set is further reduced to the 2960
examples that can be provably derived from the test data using
Table~\ref{tab:jointable}.

% TODO: Say more about symmetry?
We trained versions of both the RNN model and the RNTN model on these
data sets. In both cases, the models were implemented exactly as
described in Section~\ref{methods}, but since the items being compared
are single terms rather than full tree structures, the composition
layer was not involved, and the two models differed only in which
layer function was used for the comparison layer. We simply present
the models with two single terms, each of which corresponds to a
single vector in the (randomly initialized) vocabulary matrix $V$,
ensuring that the model has no information about the terms being
compared except the relations between them. We found that the RNTN
model worked best with 11-dimensional vector representations for the
80 sets and a 90-dimensional feature vector for the classifier. This
model was able to correctly label 99.3\% of the derivable test
examples, and 99.1\% of the remaining examples. The simpler RNN model
worked best with 11 and 75 dimensions, respectively, but was able to
achieve accuracies of only 90.0\% and 87.\%, respectively.

% and create a dataset consisting of the relations between every pair of
% sets, yielding 6400 pairs. 3200 of these pairs are then chosen as a
% test dataset, and that test dataset was further split into the 2960
% examples that can be provably derived from the test data using
% MacCartney and Manning's join table (or by the symmetry of the
% relations in about half of the cases) and the 240 that
% cannot. % TODO: Say more about symmetry?

% We tested a version of both the RNN model and the RNTN model on these
% data. In both cases, the models were implemented exactly as described
% in Section~\ref{methods}, but since the items being compared are
% single terms rather than full tree structures, the composition layer
% was not used, and the two models differed only in which layer function
% was used for the comparison layer. We found that the RNTN model worked
% best with 11 dimensional vector representations for the 80 sets and a
% 90 dimensional feature vector for the classifier. This model was able
% to correctly label 99.3\% of the derivable test examples, and 99.1\%
% of the remaining examples. The simpler RNN model worked best with 11
% and 75 dimensions, respectively, but was able to achieve accuracies of
% only 90.0\% and 87.\%, respectively.

% TODO: T-sne the RNTN model's embeddings.

These results are fairly straightforward to interpret. The RNTN model
was able to accurately encode the relations between the terms in the
geometric relations between their vectors, and was able to then use
that information to recover relations that were not overtly included
in the training data. In contrast, the RNN model was able to
approximate this behavior only incompletely. It is possible but not
likely that it could be made to find a good solution with further
optimization on different learning algorithms, or that it would do
better on a larger universe of sets for which there would be a larger
set of training data to learn from, but the RNTN is able to achieve
these effects in the well-studied framework of Section~\ref{methods}.

