\section{Recursive structure}

\begin{figure}[t]
\begin{center}
\begin{tabular}{lll}
$a\equiv a$		&~~~&	$(c~(and~(not~d)))~\#~f$\\
$b~\#~c$			&~~~&	$(not~(c~(or~b)))~\sqsubset~(not~c)$\\
$d\natneg(not~d)$	&~~~&	$f~\#~((c~(or~(not~d)))~(and~a))$\\
$(c~(and~d))\sqsubset d$&~~~&$d\sqsupset((d~(or~d))~(and~(not~b)))$\\
\end{tabular}
\end{center}

\caption{Some sample randomly generated pairs of propositional logic statements.  \label{prop-figure}} 
\end{figure}

% TODO: Cite Chomsky/Hauser/Fitch?

Recursive structure is a prominent  feature of natural language. Consider, for example, \ii{Alice said hello}, \ii{Bob said that Alice said hello}, and \ii{Carl thinks that Bob said that Alice said hello}. Overt recursion of this kind is easy to find, and theoretical accounts of natural language syntax and semantics rely heavily on recursive structures.
In order for a model to be able to accurately learn natural language meanings, then, we expect that it would need to be able to learn to represent the meanings of function words in a such a way that they are able to behave correctly when taking their own outputs as input.

% Goal: Deep recursion!

We again test this phenomenon within the framework of MacCartney and Manning-style entailment reasoning, but we replace the unanalyzed symbols from the previous experiment with expressions that involve recursive structure. To define these expressions, we turn to propositional logic, a relatively simple logic in which each variable represents either \ii{true} or \ii{false}. We generate data of the form seen in Figure \ref{prop-figure}: strings of arbitrary length consisting of six elementary proposition variables and the operators \ii{and}, \ii{or}, and \ii{not}, arranged in pairs with the logical relations between them specified. 



% TODO: Worth explicitly calling this project theorem proving? Yes! There was some confusion at CSLI.

Socher et al. \cite{socher2012semantic} have previously demonstrated the learning of a logic in a matrix-vector RNN model somewhat similar to our own, but the logic discussed here is substantially stronger, and a much better approximation of the kind of structure that is needed for natural language. The logic learned in that experiment is boolean, wherein the atomic symbols are simply the values 0 and 1, rather than variables over those values. While learning the operators of that logic is not trivial, the ouptuts of each operator can be represented accurately by a single bit. The statements of propositional logic learned here describe conditions on the truth values of propositions where those truth values are not known. As opposed to the two-way contrasts seen in \cite{socher2012semantic}, this logic distinguishes between 64 (2^6) possible assignments of truth values, and expressions of this logic define arbitrary conditions on these possible assignments, for a total of 2^{64} ($\approx 10^{20}$) possible statements that the recursive model needs to be able to distinguish. To frame this distinction in another way, the relational statements in our data our theorems about the relations between statements in the logic tested in \cite{socher2012semantic}.

We randomly generate pairs of parentheses-bracketed statements of the logic and then randomly divide the results into training and test data sets. To compute the relation between each pair of statements, we exhaustively enumerate the sets of assignments of truth values to proposition variables that would satisfy each of the statements and then convert the set-theoretic relation between those assignments into one of the seven relations. 
If we do not implement any constraint that the two statements being compared are similar in any way, the generated data consists in large part of statements in which the two refer to largely separate subsets of the six variables, and to which we will nearly always assign the \# relation. In an effort to balance the distribution of relation labels without departing from the basic task of modeling propositional logic, we disallow individual pairs of statements from referring to more than four of the six proposition variables. 

We deduplicate the data and discard pairs in which either statement is a tautology or contradiction (a statement that is true of either all or no possible assignments), for which none of the seven relation labels can accurately apply. We then divide the generated pairs into size bins based on the number of logical operators (\ii{and}, \ii{or}, or \ii{not}) in the larger of the two pairs being compared, and discard examples of size greater than twelve by this measure. Finally, we randomly sample 15\% of each bin for a held out test set.
...
65k train  

 compute the relaitons between them, discard  , and consist of about 248k training examples and 44k test examples.

 Each of the six symbols and each of the three operators is treated as a word for the purposes of our model, and is represented by a randomly initialized vector representation.





...


% TODO: Add figure for results

train up to depth 5
test up to depth 12

% Describe data

\begin{figure}[t]
\begin{center}
...\\
...\\
...\\
...\\
...\\
...\\
...\\
...\\
...\\
\end{center}

\caption{Model performance by expression size. The vertical dashed line indicates the size of the largest expressions included in the training data.  \label{prop-figure}} 
\end{figure}


The RNN model was approximately optimal with N dimensional word representations and an M dimensional comparison layer. The RNTN was approximately optimal with N and M dimensions, respectively.

% TODO: Include dimensionality

