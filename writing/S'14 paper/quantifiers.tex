\section{Reasoning with natural language quantifiers and negation}\label{sec:quantifiers}

We have seen that the RNTN can learn an approximation of propositional
logic.  However, natural languages can express functional meanings of
considerably greater complexity than this.  As a first step towards
investigating whether our models can capture this complexity, we now
attempt to directly measure the degree to which RNNs are able to
develop suitable representations for the semantics of natural language
quantifiers like \ii{some} and \ii{all}. Quantification is far from
the only place in natural language where complex functional meanings
are found, but it is a natural starting point, since it can be tested
in sentences whose structures are otherwise quite simple, and since it
has formed a standard case study in prior formal work on natural
language inference.

% \subsection{Data}

This experiment replicates similar work described in
\cite{bowman2013can}, which found that RNTNs can learn to reason well
with quantifier meanings given sufficient training data. This paper
replaces the partially manually annotated data in that paper with data
that is generated directly using the logical system that we hope to
model, yielding results that we believe to be substantially more
straightforward to interpret.

\paragraph{Experiments}
Our experimental data consist of pairs of sentences generated from a
small artificial grammar. Each sentence contains a quantifier, a noun
which may be negated, and an intransitive verb which may be
negated. We use the basic quantifiers \ii{some}, \ii{most}, \ii{all},
\ii{two}, and \ii{three}, and their negations \ii{no}, \ii{not-all},
\ii{not-most}, \ii{less-than-two}, and \ii{less-than-three}. We also
include five nouns, four intransitive verbs, and the negation symbol
\ii{not}. In order to be able to define relations between sentences
with differing lexical items, we define the lexical relations between
each noun--noun pair, each verb--verb pair, and each
quantifier--quantifier pair. The grammar accepts aligned pairs of
sentences of this form and calculates the natural logic relationship
between them.  Some examples of these data are provided in
Table~\ref{examplesofdata}.  As in previous sections, the goal of
learning is then to assign these relational labels accurately to
unseen pairs of sentences.

%nouns = ['warthogs', 'turtles', 'mammals', 'reptiles', 'pets']
%verbs = ['walk', 'move', 'swim', 'growl']
%dets = ['all', 'not_all', 'some', 'no', 'most', 'not_most', 'two', 'lt_two', 'three', 'lt_three']
%adverbs = ['', 'not']

% To assign relation labels to sentence pairs, we built a small
% task-specific implemenation of MacCartney's logic that can
% accurately label sentences of this restricted language. The logic is
% not able to derive all intuitively true relations of this language,
% and fails to derive a single unique relation for certain types of
% statement, including De Morgean's laws (e.g. \ii{(all pets) growl
% $\natneg$ (some pet) (not growl)}), and we simply discard these
% examples. Exhaustively generating the valid sentences under this
% grammar and choosing those to which a relation label can be assigned
% yields 66k sentence pairs. Some examples of these data are provided
% in Table~\ref{examplesofdata}.

\begin{table}[htp]
  \centering
  \begin{tabular}{l c l}
    \toprule
    (most warthogs) walk         & $\natneg$ & (not-most warthogs) walk\\
    (most mammals) move          & $\natind$ &  (not-most (not turtles)) move\\
    (most (not pets)) (not swim) & $\natrev$ & (not-most (not pets)) move 
    \\[2ex]    
    (no turtles) (not growl)     & $\natalt$ & (no turtles) (not swim)\\
    (no warthogs) swim           & $\natrev$ & (no warthogs) move\\
    (no warthogs) move           & $\natfor$ & (no (not reptiles)) swim\\
    \bottomrule
  \end{tabular}
  \caption{Sample data involving two different quantifier pairs.}
  \label{examplesofdata}
\end{table}

We evaluate the model using two experimental settings. In the simpler
setting, \textsc{all split}, we randomly sample 85\% of the data and evaluate on the
remaining 15\%. In this setting, the model is being asked to learn a
complete reasoning system for the limited language and logic presented
in the training data, but it is not being asked to generalize to test
examples that are substantially different from those it was trained
on. Crucially though, to succeed on this task, the model must be able
to recognize all of the lexical relations between the nouns, verbs,
and quantifiers and how they interact. For instance, it might see
\eqref{p1} and \eqref{p2} in training and, from that information,
determine \eqref{p3}.
%
% do not allow a blank line --- adds too much space
%
\begin{gather}
  \text{(most turtle) swim} \natalt \text{(no turtle) move}\label{p1}
  \\
  \text{(all lizard) reptile} \natfor  \text{(some lizard) animal}\label{p2}
  \\
  \text{(most turtle) reptile} \natalt \text{(no turtle) animal}\label{p3}
\end{gather}
%
% do not allow a blank line --- adds too much space
%
While our primary interest is in discovering the extent to which our
models can learn to encode the logic given an arbitrary amount of
data, we are also interested in the degree to which they can infer a
correct representation for the logic from more constrained training
data. To this end, we segment the sentence pairs according to which
quantifiers appear in each pair, and then hold out one such pair for
testing. We hypothesize that a model that can efficiently learn to
represent a logic should be able to construct an accurate
representation of each held out quantifier from the way that it
interacts with the other nine quantifiers which are not held
out. Since running this experiment requires choosing a pair of
quantifiers to hold out before training, the resource demands of
training prevent us from testing each of the 55 possible possible
pairs of quantifiers, and we choose only four pairs to test on.  Three
of these (\ii{two}/\ii{less-than-two}, \ii{not-all}/\ii{not-most}, and
\ii{all}/\ii{some}) were chosen because they allow for the most
different class labels at in the training data. The fourth is a
self-pair (\ii{no}/\ii{no}), meant to test that the model correctly
handles equality.

\begin{table}[tp]
  \centering
  \begin{tabular}{ l rrr }
    \toprule
    Data & Most frequent class & 20 dim RNN  & 20 dim RNTN\\
    \midrule
    \textsc{all split}	& 35.4 &	69.4 (63.2)&	\textbf{100.0 (100.0)}
    \\[1ex]    
    \textsc{pair two/less-than-two}	& 59.8 &	86.7 (68.4) &	\textbf{100.0 (100.0)} \\
    \textsc{pair not-all/not-most}	&0 & 66.0* (54.3*) &	\textbf{93.8 (91.5)} \\
    \textsc{pair all/some}	& 0& 69.7 (72.0)  &	\textbf{78.4 (84.7)} \\
    \textsc{pair no/no}	& 30.8 &	68.1 (61.7) &	\textbf{99.9 (99.7)} \\
    \bottomrule
  \end{tabular}
  \caption{Performance on the quantifier experiments. Results are reported as accuracy scores followed by macroaveraged F1 scores in parentheses.}
  \label{resultstable}
\end{table} 


\paragraph{Results} 
The results for these experiments are shown in
Figure~\ref{resultstable}. We compare the results to a most frequent
class baseline, which reflects the frequency in the test data of most
frequent class in the training data, $\natind$.  After some
cross-validation, we chose 20-dimensional word and phrase
representations, and 75 dimensional feature vectors for the
classifier.

The RNN performed poorly at this task, even though the sentences used
in these examples are short enough to avoid the pathology shown in
Figure~\ref{prop-falloff}.  However, the perfect performance by the
RNTN on the \textsc{pair no/no} experiment and its strong performance
on \textsc{all split} and \textsc{pair two/less-than-two} suggests
that that this stronger model is able to handle quantifiers correctly
given sufficient training data, but the sub-90\% results on two of the
training settings suggest that it may not be able to generalize from
data this impoverished in general. However, though the question of how
much data is necessary to accurately capture quantifier behavior in a
na\"ive model remains open, the fact that both models perform far
above baseline is promising.