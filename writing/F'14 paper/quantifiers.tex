\section{Reasoning with natural language quantifiers and negation}\label{sec:quantifiers}

We have seen that the RNTN can learn an approximation of propositional
logic.  However, natural languages can express functional meanings of
considerably greater complexity than this.  As a first step towards
investigating whether our models can capture this complexity, we now
attempt to directly measure the degree to which they are able to
develop suitable representations for the semantics of natural language
quantifiers like \ii{some} and \ii{all}. Quantification is far from
the only place in natural language where complex functional meanings
are found, but it is a natural starting point, since it can be tested
in sentences whose structures are otherwise quite simple, and since it
has formed a standard case study in prior formal work on natural
language inference \cite{Icard:Moss:2013:LILT}.

% TODO: We generate clean data from a grammar.


\paragraph{Experiments}
Our experimental data consist of pairs of sentences generated from a
small artificial grammar. Each sentence contains a quantifier, a noun
which may be negated, and an intransitive verb which may be
negated. We use the basic quantifiers \ii{some}, \ii{most}, \ii{all},
\ii{two}, and \ii{three}, and their negations \ii{no}, \ii{not-all},
\ii{not-most}, \ii{less-than-two}, and \ii{less-than-three}. We also
include five nouns, four intransitive verbs, and the negation symbol
\ii{not}. In order to be able to define relations between sentences
with differing lexical items, we define the lexical relations between
each noun--noun pair, each verb--verb pair, and each
quantifier--quantifier pair. The grammar accepts aligned pairs of
sentences of this form and calculates the natural logic relationship
between them.  Some examples of these data are provided in
Table~\ref{examplesofdata}.  As in previous sections, the goal of
learning is then to assign these relational labels accurately to
unseen pairs of sentences.

%nouns = ['warthogs', 'turtles', 'mammals', 'reptiles', 'pets']
%verbs = ['walk', 'move', 'swim', 'growl']
%dets = ['all', 'not_all', 'some', 'no', 'most', 'not_most', 'two', 'lt_two', 'three', 'lt_three']
%adverbs = ['', 'not']

% To assign relation labels to sentence pairs, we built a small
% task-specific implemenation of MacCartney's logic that can
% accurately label sentences of this restricted language. The logic is
% not able to derive all intuitively true relations of this language,
% and fails to derive a single unique relation for certain types of
% statement, including De Morgean's laws (e.g. \ii{(all pets) growl
% $\natneg$ (some pet) (not growl)}), and we simply discard these
% examples. Exhaustively generating the valid sentences under this
% grammar and choosing those to which a relation label can be assigned
% yields 66k sentence pairs. Some examples of these data are provided
% in Table~\ref{examplesofdata}.

\begin{table*}[htp]
  \centering
  \begin{tabular}{lcl}
    \toprule
    (all turtles) walk      &    $\natind$  & (less-than-three turtles) walk\\
    (three warthogs) move  &        $\natneg$ & (less-than-three warthogs) move\\
    (all mammals) (not move) &         $\natalt$ & (some warthogs) swim\\
    (some (not pets)) swim     &     $\natcov$  & (all (not pets)) (not move)\\
    \bottomrule
  \end{tabular}
  \caption{\label{examplesofdata}Examples of pairs of artificial setences with quantifiers.}

\end{table*}

% TODO: Rewrite experimental setup.
We evaluate the model using two experimental settings. In the simpler
setting, \textsc{all split}, we randomly sample 85\% of the data and evaluate on the
remaining 15\%. In this setting, the model is being asked to learn a
complete reasoning system for the limited language and logic presented
in the training data, but it is not being asked to generalize to test
examples that are substantially different from those it was trained
on. Crucially, though, to succeed on this task, the model must be able
to recognize all of the lexical relations between the nouns, verbs,
and quantifiers and how they interact. For instance, it might see
\eqref{p1} and \eqref{p2} in training and, from that information,
determine \eqref{p3}.
%
% do not allow a blank line --- adds too much space
%
\begin{gather}
  \text{(most turtle) swim} \natalt \text{(no turtle) move}\label{p1}
  \\
  \text{(all lizard) reptile} \natfor  \text{(some lizard) animal}\label{p2}
  \\
  \text{(most turtle) reptile} \natalt \text{(no turtle) animal}\label{p3}
\end{gather}
%
% do not allow a blank line --- adds too much space
%
In this configuration the model ...

\begin{table}[tp]
  \centering
  \setlength{\tabcolsep}{10pt}
  \begin{tabular}{ r@{ \ }r r@{ \ }r r@{ \ }r }
    \toprule
    \multicolumn{2}{c}{$\natind$ only} & \multicolumn{2}{c}{16 dim RNN}  & \multicolumn{2}{c}{20 dim RNTN}\\
    \midrule
    35.4 &(7.5) &	67.4&(56.5)& \textbf{100.0} & \textbf{(100.0)}\\
    \bottomrule
  \end{tabular}
  \caption{Performance on the quantifier experiments.}
  \label{qresultstable}
\end{table} 


\paragraph{Results} % TODO: Rewrite!
The results for these experiments are shown in
Table~\ref{qresultstable}. We compare the results to a most frequent
class baseline, which reflects the frequency in the test data of the
most frequent class in the training data, $\natind$.  After some
cross-validation, we chose 16- and 20-dimensional representations for
the RNN and RNTN, respectively, and 75-dimensional feature vectors for
the classifier. Training
accuracy was near 80\% for the each of the five RNN experiments, and 
near 98\% for each of the five RNTN experiments.

Discussion discussion. 
Discussion discussion. 
Discussion discussion. 
Discussion discussion. 
Discussion discussion. 
Discussion discussion. 
Discussion discussion. 
Discussion discussion. 
Discussion discussion.
