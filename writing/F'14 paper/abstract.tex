\begin{abstract}
Recursive neural network models for sentence meaning have been successful in many tasks, but it remains an open question whether any such model can learn compositional semantic representations that support logical deduction. We pursue this question directly by evaluating whether two such models can correctly learn to identify logical relationships such as entailment and contradiction between sentences. We first describe a recursive neural tensor network model for entailment and demonstrate it on the SICK challenge. We then use artificial data from a logical grammar to test the model's ability to learn to handle basic relational reasoning, recursive functions, and quantification. The model performs competitively on the SICK data and generalizes well in all three experiments on simulated data, suggesting that it can learn suitable representations for logical inference in natural language.
\end{abstract}

% TODO: Is there anything I should add that would be more specific than 'many tasks'? 'Tasks like paraphrase detection'?	