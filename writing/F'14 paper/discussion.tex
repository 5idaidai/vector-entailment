\section{Discussion and conclusion}\label{sec:discussion}

This paper first evaluates two recursive models on a series of three increasingly
challenging interpretive tasks involving natural language inference---the 
core relational algebra of natural logic with entailment and
exclusion; recursive propositional logic structures; and statements
involving quantification and negation. We then showed that the same models can learn to
perform an entailment task on natural language image captions. \mynote{The results suggest that RNTNs,
and potentially also RNNs}, have the capacity to model these tasks with 
reasonably-sized training sets. These positive results are
promising for the future of learned representation models in the
applied modeling of compositional semantics.

Of course, challenges remain. Even
the RNTN falls short of perfection in the recursion experiment, with
performance falling off steadily as the size of the expressions grows. It
remains to be seen whether these deficiencies can be overcome with
stronger models or optimization procedures. The space of possible stronger
models is sizable, including extensions of the recursive models used here
\cite{sochergrounded,kalchbrenner2014convolutional,irsoydeep}, recurrent
models \cite{sutskever2014sequence}, and even learning-enabled variants 
of more structured models like those of \newcite{grefenstette2013towards} or \newcite{rocktaschellow}.
In addition,
there remain subtle questions about how to fairly assess whether these
models have truly generalized in the way we want them to. There is a
constant tension between showing the models training data that gives
them a chance to learn the target logical functions and revealing the
answer to them in a way that leads to overfitting. The underlying
logical theories provide only limited guidance on this point.

Our artificial data experiments have only scratched the surface 
of the logical complexity of natural language; in future experiments, we hope to test sentences
with embedded quantifiers, multiple interacting quantifiers, relative
clauses, and other kinds of recursive structure. Nonetheless, the
rapid progress the field has made with these models in recent years
provides ample reason to be optimistic that they can be trained to
meet all the challenges of natural language semantics.

% These experiments represent one of the first attempts to reproduce any large fragment of the behavior of a complex logic within a neural network model, and the first attempt that we are aware of to address either the encoding of lexical relations or the learning of recursive operators. This presents considerable challenges in evaluating the particular models that we choose, since we cannot rely on prior results to establish that any particular amount or type of training data is sufficient to teach any model the structure of the logic. The positive results that we have found, however, are extremely promising for the future of learned representation models in the applied modeling of meaning. We have seen that recursive neural tensor networks are able to encode lexical relations accurately and encode recursive operators. We have also seen that both RNNs and RNTNs are able to handle the meanings of quantifiers in an inference setting in at least some cases. 

% There is ample room to build on these results. In the interest of fully mirroring the capacity of existing natural logics in learned models, it would be valuable to extend these experiments to cover other ways in which meanings are encoded in natural language, including challenges such as reasoning over sentences with transitive verbs or relative clauses. In addition, it would be highly informative to compare these results on standard recursive neural networks with other proposed learned models for sentence meaning, such as dependency tree RNNs \cite{sochergrounded}, Belief Propagation RNNs (-), or convolutional RNNs \cite{kalchbrenner2014convolutional}.

