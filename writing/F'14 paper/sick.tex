\section{The SICK textual entailment challenge}

The tree pair model architecture that we use is novel, and though the underlying recursive approach has been validated elsewhere, there is no guarantee that this architecture is suitable for handling inference in the face of the noisy labels and diverse range of linguistic structures seen in typical natural language data, and our experiments on artificial data are not alone sufficient to show this. To investigate the ability of the model to learn on natural data, we train a version of our model on the SICK textual entailment challenge corpus \cite{marelli2014sick}. The corpus consists of about 10k natural language sentence pairs (generated using data expansion from a smaller set), labeled with \ii{entailment}, \ii{contradiction}, or \ii{neutral}. At only a few thousand distinct sentences (many of them variants on an even smaller set of template sentences), the corpus is not large enough to train a high quality learned model of general natural language, but it is the largest hand labeled entailment corpus that we are aware of.

\begin{table*}[htp]
  \centering\small
  \begin{tabular}{lcl}
    \toprule
A woman is not boiling shrimps.& \ii{contradiction}&	A woman is boiling shrimps.\\
A wild deer is jumping a fence. &\ii{entailment}	&A deer is jumping a fence.\\
A potato is being sliced by a woman. &\ii{entailment}	&A woman is slicing a potato.\\
The plane, which is south African, is flying in a blue sky.& \ii{entailment}&	An airplane is flying through the air.\\
    \bottomrule
  \end{tabular}
  \caption{\label{examplesofsickdata}Examples of successful classifications on SICK.}
\end{table*}

Adapting to this task required us to make a few additions to the techniques discussed in Section \ref{methods}. In order to better handle rare words, we initialized our word embeddings using the 200 dimensional vectors trained with 
GloVe \cite{pennington2014glove} on data from Wikipedia. Since 200 dimensional vectors are too large to be practical in an RNTN, especially on a small dataset, a new embedding transformation layer is needed. Before any embedding is used as an input to a recursive layer, it is passed through an additional $\tanh$ neural network layer with the same output dimension as the recursive layer.

We also supplemented the SICK training data\footnote{We tuned the model using performance on a held out development set, but report performance here for a version of the model trained on both the training and development data and tested on the SICK test set.} with \mynote{TODO} examples of entailment data from the Denotation Graph project \cite{hodoshimage}, a corpus of automatically labeled (and thus noisy) entailment examples over image captions, the same genre of text from which SICK was drawn. We trained a single model on data from both sources, but used a separate set of softmax parameters for classifying into the labels from each source. We parsed the data from both sources with the Stanford PCFG Parser v. 3.3.1 \cite{klein2003accurate}. We also found that we were able to train a working model much more quickly with an additional technique: we collapse subtrees that were identical across both sentences in a pair down to a single head word. Finally, in order to improve regularization on the noisier data, we used dropout \cite{hinton2012improving} at the input to the comparison layer (10\%) and at the output from the embedding transform layer (25\%). 

% TODO: JMLR version of hinton paper.
% TODO: Add Karpathy to TACL paper

\begin{table}[tp]
  \centering \small
  \begin{tabular}{ l r@{ \ } r@{ \ } r@{ \ } }
    \toprule
    ~&\multicolumn{1}{c}{$\natind$ only} & \multicolumn{1}{c}{50d RNN}  & \multicolumn{1}{c}{50d RNTN}\\
    \midrule
    Train &- &- &-  \\
    Test & 56.7 &	...& \textbf{75.5} \\
    \bottomrule
  \end{tabular}
  \caption{\mynote{Performance on SICK. Training figures include performance on the Denotation Graph data.}}
  \label{sresultstable}
\end{table} 

Despite the small amount of high quality training data available and the lack of resources for learning lexical relationships (especially exclusion, as in \ii{cat}--\ii{dog}), it is possible to train our model to perform competitively on textual entailment. Our performance did not reach the state of the art (84.6\%), but exceeded the median submission to the competition (77.06\%) in a field including many models which used sophisticated hand-engineered features and lexical resources specific to the version of the entailment task at hand.

Table \ref{examplesofsickdata} shows a few typical examples of correct classifications. The model is able to successfully recognize negation and the insertion and deletion of words, as demonstrated in the first two examples. The latter two show that the model can recognize entailments and contradictions between sentences which are not superficially similar, such as passive--active pairs and sentences with low lexical overlap.