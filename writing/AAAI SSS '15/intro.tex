\subsection*{Introduction}\label{sec:intro}
  % TODO: Trim and revise
  % Less focus on RNTNs, more on embedding spaces

Natural logic offers a powerful \emph{relational} conception of
semantics: the meanings for expressions are given, at least in part,
by their inferential connections with other expressions
\cite{vanBenthem08NATLOG,maccartney2009extended}. For instance,
\word{turtle} is analyzed, not primarily by its extension in the
world, but rather by its lexical network: it entails \word{reptile},
excludes \word{chair}, is entailed by \word{sea
  turtle}, and so forth. With generalized notions of entailment and
contradiction, these relationships can be defined for all lexical
categories as well as complex phrases, sentences, and even texts. The
resulting theories of meaning offer valuable new analytic tools for
tasks involving database inference, relation extraction, and textual
entailment.

Natural logic aligns well with distributed (e.g., vector)
representations, which also naturally model meaning relationally.
Distributed representations have been used successfully in a wide
array of sophisticated language tasks. %,
% including sentiment analysis, analogy completion, relation
% extraction, and named entity recognition.
However, it remains an open question whether it is possible to train
such representations to support the rich, diverse logical reasoning
captured by natural logic; while they excel at
synonymy (similarity), the results are more mixed for entailment,
contradiction, and mutual consistency.  Using the natural logic of
\cite{maccartney2009extended} as our formal model, we address this
open question for two classes of neural model: plain neural networks
and neural tensor networks (NTNs; \cite{socher2013acl1}). The natural
logic is built from the seven relations defined in
Table~\ref{b-table}. Its formal properties are now well-understood
\cite{Icard:Moss:2013,Icard:Moss:2013:LILT}, so it provides a rigorous
set of goals for our neural models. To keep the discussion managable,
we limit attention to experiments involving the lexicon; for a more
extended treatment of complex expressions involving logical
connectives and quantifiers, see \citet{Bowman:Potts:Manning:2014}.

In our experiments, we evaluate these models' ability to learn the
basic algebra of natural logic relations from simulated data and from
the WordNet noun graph. The simulated data help us to achieve analytic
insights into what the models learn, and WordNet data show how they
fare in the real world.  We find that the plain RNN achieves only
mixed results in these experiments, whereas the stronger RNTN model
generalizes well in both cases, suggesting that it has in fact
learned, or at least learned to simulate, our target logical concepts.

% In our first experiment, we use the pre-specified logical grammar to
% generate controlled data sets and assess the ability of the two
% classes of neural network to learn the core underlying relational
% algebra from this simulated data. In our second experiment, we define
% a simple modification of the logical grammar so that it aligns with
% the WordNet noun graph, and we again assess our two classes of RNN on
% this data. The controlled data of our first experiment helps us to
% achieve analytic insights into what the models are able to learn, and
% the large, diverse lexicon of the second experiment shows us how they
% fare in the real world. 





% These experiments differentiate the increased power of RNTNs better
% than previous work and provide the most convincing demonstration to
% date of the ability of neural networks to model semantic inferences
% in complex natural language sentences.

% TODO: Add citations on related work in structuring relations in embedding spaces:
% - Representing partâ€“whole relations in conceptual spaces
% - Something McCallum/Universal Schema?
