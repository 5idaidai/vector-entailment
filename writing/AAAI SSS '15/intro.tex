\subsection*{Introduction}\label{sec:intro}
  % TODO: Trim and revise
  % Less focus on RNTNs, more on embedding spaces

Supervised recursive neural network models (RNNs) for sentence meaning
have been successful in an array of sophisticated language tasks,
including sentiment analysis \cite{socher2011semi,socher2013acl1},
image description \cite{sochergrounded}, and paraphrase detection
\cite{Socher-etal:2011:Paraphrase}. These results are encouraging for
the ability of these models to learn compositional semantic grammars,
but it remains an open question whether they can achieve the same
results as grammars based in logical forms
\cite{Warren:Pereira:1982,Zelle:Mooney:1996,ZetCol:2005,LiangJordan:2013}
when it comes to the core semantic concepts of quantification,
entailment, and contradiction needed to identify the relationships
between sentences like \ii{every animal walks}, \ii{every turtle
  moves}, and \ii{most reptiles don't move}. To date, experimental
investigations of these concepts using distributed representations
have been largely confined to short phrases
\cite{Mitchell:Lapata:2010,Grefenstette-etal:2011,baroni2012entailment,kalchbrenner2014convolutional}.
For robust natural language understanding, it is essential to model
these phenomena in their full generality on complex linguistic
structures.

We address this question in the context of \ii{natural language
  inference} (also known as \ii{recognizing textual entailment};
\cite{dagan2006pascal}), in which the goal is to determine the core
inferential relationship between two sentences. Much of the
theoretical work on this task (and some successful implemented models
\cite{maccartney2009extended,watanabe2012latent}) involves \ii{natural
  logics}, which are formal systems that define rules of inference
between natural language words, phrases, and sentences without the
need of intermediate representations in an artificial logical
language. Following \cite{bowman2013can}, we use the natural logic
developed by \cite{maccartney2009extended} as our formal model. This
logic defines seven core relations of synonymy, entailment,
contradiction, and mutual consistency, as summarized in
Table~\ref{b-table}, and it provides rules of semantic combination for
projecting these relations from the lexicon up to complex phrases. The
formal properties and inferential strength of this system are now
well-understood \cite{Icard:Moss:2013,Icard:Moss:2013:LILT}.

In our experiments, we use this pre-specified logical grammar to
generate controlled data sets encoding the semantic relationships
between pairs of expressions and evaluate whether each of two classes
of neural model --- plain RNNs and recursive neural tensor networks
(RNTNs, \cite{socher2013acl1}) --- can learn those relationships
correctly. In our first experiment (Section~\ref{sec:join}), we
evaluate the ability of these models to learn the core relational
algebra of natural logic from data. Our second experiment
(Section~\ref{sec:recursion}) extends this evaluation to relations
between complex recursive structures like $(a \plor b)$ and
$\plneg(\plneg a \pland \plneg b)$, and our third experiment
(Section~\ref{sec:quantifiers}) involves relations between quantified
statements like \ii{every reptile walks} and \ii{no turtle moves}.
We find that the plain RNN achieves only mixed results in all three
experiments, whereas the stronger RNTN model generalizes well in every
case, suggesting that it has in fact learned, or at least learned to
simulate, our target logical
concepts. These experiments differentiate the increased power of RNTNs
better than previous work and provide the most convincing
demonstration to date of the ability of neural networks to model
semantic inferences in complex natural language sentences.

\begin{table}[tp]
  \centering
  \setlength{\tabcolsep}{15pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{l c l l} 
    \toprule
    Name & Symbol & Set-theoretic definition & Example \\ 
    \midrule
    entailment         & $x \natfor y$   & $x \subset y$ & \ii{turtle, reptile}  \\ 
    reverse entailment & $x \natrev y$   & $x \supset y$ & \ii{reptile, turtle}  \\ 
    equivalence        & $x \nateq y$    & $x = y$       & \ii{couch, sofa} \\ 
    alternation        & $x \natalt y$   & $x \cap y = \emptyset \wedge x \cup y \neq \mathcal{D}$ & \ii{turtle, warthog} \\ 
    negation           & $x \natneg y$   & $x \cap y = \emptyset \wedge x \cup y = \mathcal{D}$    & \ii{able, unable} \\
    cover              & $x \natcov y$   & $x \cap y \neq \emptyset \wedge x \cup y = \mathcal{D}$ & \ii{animal, non-turtle} \\ 
    independence       & $x \natind y$   & (else) & \ii{turtle, pet}\\
    \bottomrule
  \end{tabular}
  \caption{The seven natural logic relations of \cite{maccartney2009extended}. 
    $\mathcal{D}$ is the universe of possible objects of the same type as those being compared, 
    and the relation $\natind$ applies whenever none of the other six do.} %, including when there 
    %is insufficient knowledge to choose a label.}
  \label{b-table}
\end{table}


% TODO: Add citations on related work in structuring relations in embedding spaces:
% - Representing partâ€“whole relations in conceptual spaces
% - Something McCallum/Universal Schema?
