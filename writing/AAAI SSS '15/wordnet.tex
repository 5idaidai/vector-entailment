\subsection*{Reasoning about lexical relations in WordNet}\label{sec:wordnet}

We have shown that a model can learn to reason using relations over a small artificial dataset, 
but it the question of whether the same model can show this behavior over the type and number of
relations seen in a real natural language vocabulary.
Unfortunately, we are aware of no reliable source of relation data that covers a large natural 
language vocabulary using the natural logic relations discussed above. However, WordNet 
\cite{fellbaum2010wordnet} provides a different type of lattice structure which is compatible with
our learning paradigm.

We extract three types of relation from WordNet. \ii{Hypernym} and \ii{hyponym} can be are represented
directly in the WordNet graph structure, and correspond closely to the $\natrev$ and $\natfor$ relations from
natural logic. As in natural logic, these relations are mirror images of one another: if \ii{dog} is a
hyponym of \ii{animal} (perhaps indirectly by way of \ii{canid}, \ii{mammal}, etc.), then \ii{animal} must be a 
hypernym of \ii{dog}. A third informative relation can 
be readily derived from the graph, which we call \ii{coordinate}. Coordinate terms are those which share
a direct hypernym, like \ii{dalmation}, \ii{pug}, and \ii{puppy}, which are all direct hyponyms of \ii{dog}. 
Coordinate terms tend to exclude one another (as with $\natalt$ above) \cite{Hurford:1974}, but this tendancy
is not universal. WordNet defines hypernymy and hyponymy over sets of synonyms, rather than over individual 
terms, so we do not include a \ii{synonym} or \ii{equivalent} relation, but rather simply consider only one
member of each set of synonyms.

To limit the size of the vocabulary without otherwise simplifying the learning problem, we extract all of the
instances of these three relations for those terms in WordNet that are hyponyms of the node \texttt{organism.n.01}.
In order to balance the distribution of the classes, we slightly downsample instances of the \ii{coordinate} relation,
yielding 36,772 relations among 3,217 terms. We randomly select 15\% of these relations as test data. Unlike in the
previous experiment, it is not straightforward here to determine in advance how much training data should be required
to learn an accurate model, so we performed training runs with various fractions of the training portion of the 
dataset. 

\paragraph{Results} 
We find that with only a small amount of hyperparamater tuning, our model can perform almost perfectly, and that performance remains fairly good even when the fraction of the training data used is decreased. Neither model has reached perfect performance in any of our experiments, though we are continuing to tune both models. We find that the NN and NTN both capture the required generalizations very well.

% TODO: Describe use of GloVe once there are clearer results. Since it doesn't look like it makes a huge difference,
% should we perhaps just omit mention of it entirely to save space?

\begin{table}[h]
\centering\resizebox{5.5in}{!}{
  \setlength{\tabcolsep}{15pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{l l l l l l} 
    \toprule
     Portion of training data & \multicolumn{2}{c}{NN} & \multicolumn{2}{c}{NTN} & Baseline\\
       & \multicolumn{1}{c}{w/ GloVe} & \multicolumn{1}{c}{w/o GloVe} & \multicolumn{1}{c}{w/ GloVe} & \multicolumn{1}{c}{w/o GloVe} \\
    \midrule
     100\%   & 99.5 (100.0) & 98.9 (99.7) & \textbf{99.6} (100.0) & 99.2 (100.0) & 37.9 (--)\\
     75\%   & 98.9 (100.0) & 37.9 (37.0) & 98.6 (100.0) & 98.4 (100.0) & 37.9 (--)\\
     33\%   & 96.0 (100.0) & 37.0 (37.0) & 95.4 (100.0) & 95.3 (100.0) & 37.9 (--)\\ % TODO: Rerunning. Got stuck predicting the most frequent class.
     16\%   & 91.8 (100.0) & 91.9 (100.0) & 92.1 (99.7) & 92.2 (100.0) & 37.9 (--)\\
    \bottomrule
  \end{tabular}}
 \caption{Test (train) accuracy figures on the WordNet data. The baseline figure is simply the frequency of the most frequent class, \ii{coordinate}.\label{b-table}}  
\end{table}

