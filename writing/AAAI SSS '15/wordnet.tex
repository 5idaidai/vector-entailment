\subsection*{Reasoning about lexical relations in WordNet}\label{sec:wordnet}

Using simulated data as above is reassuring about what the models
learn and why, but we also want to know how they perform with a real
natural language vocabulary. Unfortunately, as far as we are aware,
there are no available resources labeling such a vocabulary with the
relations from Table~\ref{b-table}. However, the relations in WordNet
\cite{fellbaum2010wordnet} come close and pose the same substantive
challenges.

% WordNet \cite{fellbaum2010wordnet} provides a different type of
% lattice structure which is compatible with our learning paradigm.

We extract three types of relation from WordNet. \ii{Hypernym} and
\ii{hyponym} can be are represented directly in the WordNet graph
structure, and correspond closely to the $\natrev$ and $\natfor$
relations from natural logic. As in natural logic, these relations are
mirror images of one another: if \ii{dog} is a hyponym of \ii{animal}
(perhaps indirectly by way of \ii{canid}, \ii{mammal}, etc.), then
\ii{animal} is a hypernym of \ii{dog}. We also extract \ii{coordinate}
terms, which share a direct hypernym, like \ii{dalmation}, \ii{pug},
and \ii{puppy}, which are all direct hyponyms of \ii{dog}.  Coordinate
terms tend to exclude one another, thereby providing a reasonable
approximation of exclusion. WordNet defines hypernymy and hyponymy over
sets of synonyms, rather than over individual terms, so we do not
include a \ii{synonym} or \ii{equivalent} relation, but rather
consider only one member of each set of synonyms.

To limit the size of the vocabulary without otherwise simplifying the learning problem, we extract all of the
instances of these three relations for single word nouns in WordNet that are hyponyms of the node 
\texttt{organism.n.01}. In order to balance the distribution of the classes, we slightly downsample instances 
of the \ii{coordinate} relation, yielding 36,772 relations among 3,217 terms. We randomly select 15\% of these 
relations as test data. Unlike in the previous experiment, it is not straightforward here to determine in advance
 how much training data should be required to learn an accurate model, so we performed training runs with 
 various fractions of the training portion of the 
dataset. Embeddings were fixed at 25 dimensions and were initialized either randomly or using distributional 
vectors from GloVe \cite{pennington2014glove}.

\paragraph{Results} 
We find that the NTN performs almost perfectly even when trained only
on a fraction of the ground truth relations. The plain NN is much less
successful. Neither model has reached perfect performance in any of
our experiments, though we are continuing to tune both models.

\begin{table}[h]
\centering\resizebox{5.5in}{!}{
  \setlength{\tabcolsep}{15pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabular}{r r@{ }r r@{ }r r@{ }r r@{ }r r@{ }r} 
    \toprule
     Portion of & \multicolumn{4}{c}{NN} & \multicolumn{4}{c}{NTN} & \multicolumn{2}{c}{Baseline}\\
     training data  & \multicolumn{2}{c}{w/ GloVe} & \multicolumn{2}{c}{w/o GloVe} & \multicolumn{2}{c}{w/ GloVe} & \multicolumn{2}{c}{w/o GloVe} \\
    \midrule
     100\%   & 99.5 &(100.0) & 98.9 &(99.7)    & \textbf{99.6} &(100.0) & 99.2 &(100.0) & 37.9 &(--)\\
     75\%    & 98.9 &(100.0) & 37.9 &(37.0)    & 98.6 &(100.0)          & 98.4 &(100.0) & 37.9 &(--)\\
     33\%    & 96.0 &(100.0) & 37.0 &(37.0)    & 95.4 &(100.0)          & 95.3 &(100.0) & 37.9 &(--)\\ % TODO: Rerunning. Got stuck predicting the most frequent class.
     16\%    & 91.8 &(100.0) & 91.9 &(100.0)   & 92.1 &(99.7)           & 92.2 &(100.0) & 37.9 &(--)\\
    \bottomrule
  \end{tabular}}
 \caption{Test (train) accuracy figures on the WordNet data. The baseline figure is simply the frequency of the most frequent class, \ii{coordinate}.\label{b-table}}  
\end{table}

