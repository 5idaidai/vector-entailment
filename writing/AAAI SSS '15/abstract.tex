\begin{abstract}
  Natural logic offers a powerful relational conception of meaning
  that is a natural counterpart to distributed semantic
  representations, which have proven valuable in a wide range of
  sophisticated language tasks. However, it remains an open question
  whether it is possible to train distributed representations to
  support the rich, diverse logical reasoning captured by natural
  logic. We address this question a simple neural network-based
  model for learning embeddings.  Our experiments evaluate the models' 
  ability to learn the basic algebra of natural logic relations from 
  simulated data and from the WordNet noun graph.  We find that the model
  can learn to do inference over relations with near-perfect accuracy
  from moderate amounts of training data.
\end{abstract}

