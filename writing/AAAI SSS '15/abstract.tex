\begin{abstract}
  Natural logic offers a powerful relational conception of meaning
  that fits well with recent distributed approaches to semantic
  representation. Such representations have proven valuable in a wide
  range of sophisticated language tasks, but it remains an open
  question whether it is possible to train them to support the rich,
  diverse logical reasoning captured by natural logic. We address this
  question for two classes of neural model: plain neural networks and
  neural tensor networks.  Our first experiment evaluates these
  models' ability to learn the basic algebra of natural logic
  relations from simulated data, and our second experiment evaluates
  them on the WordNet noun graph.  We find that the plain RNN achieves
  only mixed results, whereas the stronger NTN generalizes well in
  both settings and appears capable of learning suitable
  representations for natural language logical inference.
\end{abstract}

