\begin{abstract}
  Natural logic offers a powerful relational conception of meaning
  that is a natural counterpart to distributed semantic
  representations, which have proven valuable in a wide range of
  sophisticated language tasks. However, it remains an open question
  whether it is possible to train distributed representations to
  support the rich, diverse logical reasoning captured by natural
  logic. We address this question for two classes of neural model:
  plain neural networks and neural tensor networks (NTNs).  Our
  experiments evaluate these models' ability to learn the basic
  algebra of natural logic relations from simulated data and from the
  WordNet noun graph.  We find that the plain RNN achieves only mixed
  results, whereas the stronger NTN generalizes well in both settings
  and appears capable of learning suitable representations for natural
  language logical inference.
\end{abstract}

